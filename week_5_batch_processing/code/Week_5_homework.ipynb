{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18199204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43842bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/abura/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/06/24 08:52:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/24 08:52:49 WARN SparkContext: Please ensure that the number of slots available on your executors is limited by the number of cores to task cpus and not another custom resource. If cores is not the limiting resource then dynamic allocation will not work properly!\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('spark://de-zoomcamp.europe-west6-a.c.de-zcamp-1234.internal:7077') \\\n",
    "    .appName('Homework') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fed632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hvfhw = spark.read.parquet('fhvhv_tripdata_2021-02.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fda1b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: timestamp (nullable = true)\n",
      " |-- on_scene_datetime: timestamp (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_time: long (nullable = true)\n",
      " |-- base_passenger_fare: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- bcf: double (nullable = true)\n",
      " |-- sales_tax: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- driver_pay: double (nullable = true)\n",
      " |-- shared_request_flag: string (nullable = true)\n",
      " |-- shared_match_flag: string (nullable = true)\n",
      " |-- access_a_ride_flag: string (nullable = true)\n",
      " |-- wav_request_flag: string (nullable = true)\n",
      " |-- wav_match_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hvfhw.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e2a96",
   "metadata": {},
   "source": [
    "#### Question 3. Count records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94fb0a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "367170"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_hvfhw \\\n",
    "    .withColumn('pickup_datetime', F.to_date(df_hvfhw.pickup_datetime)) \\\n",
    "    .filter(\"pickup_datetime == '2021-02-15'\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b98845f",
   "metadata": {},
   "source": [
    "#### Question 4. Longest trip for each day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26637157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:===================================================>  (190 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|pickup_date|max(duration)|\n",
      "+-----------+-------------+\n",
      "| 2021-02-11|        75540|\n",
      "+-----------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_hvfhw \\\n",
    "   .withColumn('duration', F.unix_timestamp(df_hvfhw.dropoff_datetime) - F.unix_timestamp(df_hvfhw.pickup_datetime)) \\\n",
    "   .withColumn('pickup_date', F.to_date(df_hvfhw.pickup_datetime)) \\\n",
    "   .groupBy('pickup_date') \\\n",
    "      .max('duration') \\\n",
    "   .orderBy('max(duration)', ascending=False) \\\n",
    "   .show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d58fe53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hvfhw.registerTempTable('hvfhw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0aa9e08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+\n",
      "|                day|duration|\n",
      "+-------------------+--------+\n",
      "|2021-02-11 00:00:00|   75540|\n",
      "+-------------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    date_trunc('day', pickup_datetime) AS day, \n",
    "    (CAST(dropoff_datetime AS LONG)- CAST (pickup_datetime AS LONG)) AS duration\n",
    "FROM \n",
    "    hvfhw\n",
    "ORDER BY\n",
    "    2 DESC;\n",
    "\"\"\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b0cc81",
   "metadata": {},
   "source": [
    "#### Question 5. Most frequent dispatching_base_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed3836b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:========================================>             (150 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------------+\n",
      "|dispatching_base_num|count(dispatching_base_num)|\n",
      "+--------------------+---------------------------+\n",
      "|              B02510|                    3233664|\n",
      "+--------------------+---------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 30:================================================>     (181 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_hvfhw \\\n",
    "    .groupBy('dispatching_base_num') \\\n",
    "        .agg({'dispatching_base_num':'count'}) \\\n",
    "    .orderBy('count(dispatching_base_num)', ascending=False) \\\n",
    "    .show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d0474c",
   "metadata": {},
   "source": [
    "####  Question 6. Most common locations pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "251a8758",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = spark.read.parquet('zones/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38fb321f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+------------+\n",
      "|LocationID|Borough|                Zone|service_zone|\n",
      "+----------+-------+--------------------+------------+\n",
      "|         1|    EWR|      Newark Airport|         EWR|\n",
      "|         2| Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|  Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "+----------+-------+--------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zones.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c02ee076",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hvfhw_zones = df_hvfhw \\\n",
    "    .join(zones, df_hvfhw.PULocationID==zones.LocationID) \\\n",
    "    .withColumnRenamed('Zone', 'PUZone') \\\n",
    "    .drop('service_zone', 'Borough', 'LocationID') \\\n",
    "    .join(zones, df_hvfhw.DOLocationID==zones.LocationID) \\\n",
    "    .withColumnRenamed('Zone', 'DOZone') \\\n",
    "    .drop('service_zone', 'Borough', 'LocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86c4b7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:===============================================>      (176 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|       Location_pair|count|\n",
      "+--------------------+-----+\n",
      "|East New York / E...|45041|\n",
      "+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 54:====================================================> (196 + 4) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_hvfhw_zones \\\n",
    "    .withColumn('Location_pair', F.concat(df_hvfhw_zones.PUZone, F.lit(' / '), df_hvfhw_zones.DOZone)) \\\n",
    "    .groupBy('Location_pair') \\\n",
    "        .count()\\\n",
    "    .orderBy('count', ascending=False) \\\n",
    "    .show(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
